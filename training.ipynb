{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import random\n",
    "from PIL import Image\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.transforms import RandomHorizontalFlip, RandomRotation, ColorJitter,transforms\n",
    "from FaceRecognitionModel import FaceRecognition,get_device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = 'lfw_funneled'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_folders = os.listdir(folder_path)\n",
    "data_path = []\n",
    "for path in image_folders:\n",
    "    if '.' not in path:\n",
    "        image_path = os.path.join(folder_path,path)\n",
    "    if len(os.listdir(image_path)) >= 2:\n",
    "        data_path.append(image_path)\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1680, 'lfw_funneled\\\\Aaron_Peirsol')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_path),data_path[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform1 = transforms.Compose([\n",
    "    transforms.Resize((200, 200)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Updated transform with data augmentation\n",
    "transform2 = transforms.Compose([\n",
    "    RandomHorizontalFlip(), \n",
    "    RandomRotation(10),\n",
    "    ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.Resize((200, 200)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = []\n",
    "for path1,path2 in zip(data_path[:len(data_path)-1], data_path[1:]):\n",
    "\n",
    "    images1 = os.listdir(path1)\n",
    "    images2 = os.listdir(path2)\n",
    "\n",
    "    anchor_path = os.path.join(path1,images1[0])\n",
    "    positive_path = os.path.join(path1,images1[1])\n",
    "    negative_path = os.path.join(path2,images2[0])\n",
    "\n",
    "    anchor_image = Image.open(anchor_path)\n",
    "    positive_image = Image.open(positive_path)\n",
    "    negative_image = Image.open(negative_path)\n",
    "\n",
    "    anchor = transform1(anchor_image)\n",
    "    positive = transform1(positive_image)\n",
    "    negative = transform1(negative_image)\n",
    "    training.append([anchor,positive,negative])\n",
    "\n",
    "for path1,path2 in zip(data_path[:len(data_path)-1], data_path[1:]):\n",
    "\n",
    "    images1 = os.listdir(path1)\n",
    "    images2 = os.listdir(path2)\n",
    "\n",
    "    anchor_path = os.path.join(path1,images1[0])\n",
    "    positive_path = os.path.join(path1,images1[1])\n",
    "    negative_path = os.path.join(path2,images2[0])\n",
    "\n",
    "    anchor_image = Image.open(anchor_path)\n",
    "    positive_image = Image.open(positive_path)\n",
    "    negative_image = Image.open(negative_path)\n",
    "\n",
    "    anchor = transform2(anchor_image)\n",
    "    positive = transform2(positive_image)\n",
    "    negative = transform2(negative_image)\n",
    "    training.append([anchor,positive,negative])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 200, 200]),\n",
       " torch.Size([3, 200, 200]),\n",
       " torch.Size([3, 200, 200]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training[0][0].shape,training[0][1].shape,training[0][2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3358, 839.5)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training),(len(training)*0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_size = int(len(training)*0.25)\n",
    "# testing = training[len(training)-test_size:]\n",
    "# training = training[:len(training)-test_size]\n",
    "# len(training),len(testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\python\\face_recognition1\\FaceRecognitionModel.py:106: UserWarning: expandable_segments not supported on this platform (Triggered internally at ..\\c10/cuda/CUDAAllocatorConfig.h:30.)\n",
      "  return x.to(get_device())\n"
     ]
    }
   ],
   "source": [
    "faceRecognition = FaceRecognition().to(get_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0001"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kaif Khan\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    }
   ],
   "source": [
    "criterian = nn.TripletMarginLoss(margin=1.0,reduction='mean').to(get_device())\n",
    "optimizer = torch.optim.SGD(params=faceRecognition.parameters(), lr=0.001, momentum=0.7, weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=5, factor=0.85, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 100\n",
    "BATCH_SIZE = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FaceRecognition(\n",
       "  (embedding): CNN(\n",
       "    (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv2): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv3): Conv2d(128, 256, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1))\n",
       "    (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (dropout): Dropout(p=0.2, inplace=False)\n",
       "    (linear1): Linear(in_features=30976, out_features=1024, bias=True)\n",
       "    (linear2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    (output): Linear(in_features=512, out_features=256, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "faceRecognition.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataFormate(data):\n",
    "    anchor,positive,negative = [],[],[]\n",
    "    for i in range(len(data)):\n",
    "        anchor.append(data[i][0])\n",
    "        positive.append(data[i][1])\n",
    "        negative.append(data[i][2])\n",
    "    \n",
    "    return torch.stack(anchor).to(get_device()),torch.stack(positive).to(get_device()), torch.stack(negative).to(get_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataFormate([torch.randn(3,200,200),torch.randn(3,200,200),torch.randn(3,200,200)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for epoch in range(EPOCHS):\n",
    "#     running_loss = 0.0\n",
    "\n",
    "#     for i in range(0,len(training),BATCH_SIZE):\n",
    "#         data = training[i:i+BATCH_SIZE]\n",
    "#         anchor_tensor, positive_tensor, negative_tensor = dataFormate(data)\n",
    "        \n",
    "#         anchor_tensor = anchor_tensor.to(get_device())\n",
    "#         positive_tensor = positive_tensor.to(get_device())\n",
    "#         negative_tensor = negative_tensor.to(get_device())\n",
    "\n",
    "#         anchor_embedding = faceRecognition(anchor_tensor)\n",
    "#         positive_embedding = faceRecognition(positive_tensor)\n",
    "#         negative_embedding = faceRecognition(negative_tensor)\n",
    "\n",
    "#         loss = criterian(anchor_embedding,positive_embedding,negative_embedding)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         running_loss += loss.item()\n",
    "#     avg_loss = running_loss / (len(training)//BATCH_SIZE)\n",
    "#     scheduler.step(loss)\n",
    "#     print(f\"Epoch {epoch} , loss :- {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kaif Khan\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 , loss :- 1.0098\n",
      "Epoch 2 , loss :- 1.0059\n",
      "Epoch 3 , loss :- 1.0016\n",
      "Epoch 4 , loss :- 0.9887\n",
      "Epoch 5 , loss :- 0.9691\n",
      "Epoch 6 , loss :- 0.9420\n",
      "Epoch 7 , loss :- 0.9149\n",
      "Epoch 8 , loss :- 0.8810\n",
      "Epoch 9 , loss :- 0.8519\n",
      "Epoch 10 , loss :- 0.8227\n",
      "Epoch 11 , loss :- 0.7893\n",
      "Epoch 12 , loss :- 0.7567\n",
      "Epoch 13 , loss :- 0.7319\n",
      "Epoch 14 , loss :- 0.7103\n",
      "Epoch 15 , loss :- 0.6914\n",
      "Epoch 16 , loss :- 0.6833\n",
      "Epoch 17 , loss :- 0.6752\n",
      "Epoch 18 , loss :- 0.6656\n",
      "Epoch 19 , loss :- 0.6554\n",
      "Epoch 20 , loss :- 0.6529\n",
      "Epoch 21 , loss :- 0.6383\n",
      "Epoch 22 , loss :- 0.6079\n",
      "Epoch 23 , loss :- 0.5957\n",
      "Epoch 24 , loss :- 0.6045\n",
      "Epoch 25 , loss :- 0.5901\n",
      "Epoch 26 , loss :- 0.6103\n",
      "Epoch 27 , loss :- 0.5901\n",
      "Epoch 28 , loss :- 0.5889\n",
      "Epoch 29 , loss :- 0.5969\n",
      "Epoch 30 , loss :- 0.5802\n",
      "Epoch 31 , loss :- 0.5866\n",
      "Epoch 32 , loss :- 0.5595\n",
      "Epoch 33 , loss :- 0.5579\n",
      "Epoch 34 , loss :- 0.5401\n",
      "Epoch 35 , loss :- 0.5586\n",
      "Epoch 36 , loss :- 0.5535\n",
      "Epoch 37 , loss :- 0.5504\n",
      "Epoch 38 , loss :- 0.5385\n",
      "Epoch 39 , loss :- 0.5389\n",
      "Epoch 40 , loss :- 0.5240\n",
      "Epoch 41 , loss :- 0.5013\n",
      "Epoch 42 , loss :- 0.5021\n",
      "Epoch 43 , loss :- 0.4867\n",
      "Epoch 44 , loss :- 0.5059\n",
      "Epoch 45 , loss :- 0.4894\n",
      "Epoch 46 , loss :- 0.4909\n",
      "Epoch 47 , loss :- 0.4704\n",
      "Epoch 48 , loss :- 0.4733\n",
      "Epoch 49 , loss :- 0.4752\n",
      "Epoch 50 , loss :- 0.4597\n",
      "Epoch 51 , loss :- 0.4600\n",
      "Epoch 52 , loss :- 0.4715\n",
      "Epoch 53 , loss :- 0.4276\n",
      "Epoch 54 , loss :- 0.4447\n",
      "Epoch 55 , loss :- 0.4001\n",
      "Epoch 56 , loss :- 0.4107\n",
      "Epoch 57 , loss :- 0.4355\n",
      "Epoch 58 , loss :- 0.4015\n",
      "Epoch 59 , loss :- 0.3885\n",
      "Epoch 60 , loss :- 0.4072\n",
      "Epoch 61 , loss :- 0.3726\n",
      "Epoch 62 , loss :- 0.3961\n",
      "Epoch 63 , loss :- 0.3687\n",
      "Epoch 64 , loss :- 0.3698\n",
      "Epoch 65 , loss :- 0.3950\n",
      "Epoch 66 , loss :- 0.4082\n",
      "Epoch 67 , loss :- 0.3980\n",
      "Epoch 68 , loss :- 0.3810\n",
      "Epoch 69 , loss :- 0.3798\n",
      "Epoch 70 , loss :- 0.3684\n",
      "Epoch 71 , loss :- 0.3371\n",
      "Epoch 72 , loss :- 0.3198\n",
      "Epoch 73 , loss :- 0.3331\n",
      "Epoch 74 , loss :- 0.3081\n",
      "Epoch 75 , loss :- 0.2761\n",
      "Epoch 76 , loss :- 0.2703\n",
      "Epoch 77 , loss :- 0.2779\n",
      "Epoch 78 , loss :- 0.2686\n",
      "Epoch 79 , loss :- 0.2397\n",
      "Epoch 80 , loss :- 0.2340\n",
      "Epoch 81 , loss :- 0.2308\n",
      "Epoch 82 , loss :- 0.2417\n",
      "Epoch 83 , loss :- 0.2419\n",
      "Epoch 84 , loss :- 0.2725\n",
      "Epoch 85 , loss :- 0.2432\n",
      "Epoch 86 , loss :- 0.2318\n",
      "Epoch 87 , loss :- 0.2305\n",
      "Epoch 88 , loss :- 0.2346\n",
      "Epoch 89 , loss :- 0.2391\n",
      "Epoch 90 , loss :- 0.2392\n",
      "Epoch 91 , loss :- 0.2330\n",
      "Epoch 92 , loss :- 0.2002\n",
      "Epoch 93 , loss :- 0.1915\n",
      "Epoch 94 , loss :- 0.1742\n",
      "Epoch 95 , loss :- 0.1844\n",
      "Epoch 96 , loss :- 0.1622\n",
      "Epoch 97 , loss :- 0.1751\n",
      "Epoch 98 , loss :- 0.1576\n",
      "Epoch 99 , loss :- 0.1408\n",
      "Epoch 100 , loss :- 0.1408\n",
      "Fold 1 , Validation Loss: 0.6715\n",
      "Fold 2/5\n",
      "Epoch 1 , loss :- 1.0089\n",
      "Epoch 2 , loss :- 1.0072\n",
      "Epoch 3 , loss :- 1.0030\n",
      "Epoch 4 , loss :- 0.9914\n",
      "Epoch 5 , loss :- 0.9756\n",
      "Epoch 6 , loss :- 0.9539\n",
      "Epoch 7 , loss :- 0.9330\n",
      "Epoch 8 , loss :- 0.9018\n",
      "Epoch 9 , loss :- 0.8722\n",
      "Epoch 10 , loss :- 0.8319\n",
      "Epoch 11 , loss :- 0.7979\n",
      "Epoch 12 , loss :- 0.7695\n",
      "Epoch 13 , loss :- 0.7410\n",
      "Epoch 14 , loss :- 0.7252\n",
      "Epoch 15 , loss :- 0.7189\n",
      "Epoch 16 , loss :- 0.7034\n",
      "Epoch 17 , loss :- 0.6922\n",
      "Epoch 18 , loss :- 0.6785\n",
      "Epoch 19 , loss :- 0.6735\n",
      "Epoch 20 , loss :- 0.6502\n",
      "Epoch 21 , loss :- 0.6557\n",
      "Epoch 22 , loss :- 0.6284\n",
      "Epoch 23 , loss :- 0.6367\n",
      "Epoch 24 , loss :- 0.6177\n",
      "Epoch 25 , loss :- 0.6161\n",
      "Epoch 26 , loss :- 0.6017\n",
      "Epoch 27 , loss :- 0.5920\n",
      "Epoch 28 , loss :- 0.5737\n",
      "Epoch 29 , loss :- 0.5680\n",
      "Epoch 30 , loss :- 0.5851\n",
      "Epoch 31 , loss :- 0.5724\n",
      "Epoch 32 , loss :- 0.5576\n",
      "Epoch 33 , loss :- 0.5715\n",
      "Epoch 34 , loss :- 0.5490\n",
      "Epoch 35 , loss :- 0.5548\n",
      "Epoch 36 , loss :- 0.5474\n",
      "Epoch 37 , loss :- 0.5798\n",
      "Epoch 38 , loss :- 0.5601\n",
      "Epoch 39 , loss :- 0.5414\n",
      "Epoch 40 , loss :- 0.5411\n",
      "Epoch 41 , loss :- 0.5604\n",
      "Epoch 42 , loss :- 0.5679\n",
      "Epoch 43 , loss :- 0.5679\n",
      "Epoch 44 , loss :- 0.5495\n",
      "Epoch 45 , loss :- 0.5243\n",
      "Epoch 46 , loss :- 0.5473\n",
      "Epoch 47 , loss :- 0.5364\n",
      "Epoch 48 , loss :- 0.5423\n",
      "Epoch 49 , loss :- 0.5350\n",
      "Epoch 50 , loss :- 0.5384\n",
      "Epoch 51 , loss :- 0.5633\n",
      "Epoch 52 , loss :- 0.5004\n",
      "Epoch 53 , loss :- 0.4836\n",
      "Epoch 54 , loss :- 0.4866\n",
      "Epoch 55 , loss :- 0.4448\n",
      "Epoch 56 , loss :- 0.4847\n",
      "Epoch 57 , loss :- 0.4570\n",
      "Epoch 58 , loss :- 0.5049\n",
      "Epoch 59 , loss :- 0.4517\n",
      "Epoch 60 , loss :- 0.4517\n",
      "Epoch 61 , loss :- 0.4162\n",
      "Epoch 62 , loss :- 0.4316\n",
      "Epoch 63 , loss :- 0.4280\n",
      "Epoch 64 , loss :- 0.4685\n",
      "Epoch 65 , loss :- 0.4215\n",
      "Epoch 66 , loss :- 0.4562\n",
      "Epoch 67 , loss :- 0.4644\n",
      "Epoch 68 , loss :- 0.4493\n",
      "Epoch 69 , loss :- 0.4529\n",
      "Epoch 70 , loss :- 0.4263\n",
      "Epoch 71 , loss :- 0.4042\n",
      "Epoch 72 , loss :- 0.3687\n",
      "Epoch 73 , loss :- 0.3727\n",
      "Epoch 74 , loss :- 0.3751\n",
      "Epoch 75 , loss :- 0.3575\n",
      "Epoch 76 , loss :- 0.3321\n",
      "Epoch 77 , loss :- 0.3320\n",
      "Epoch 78 , loss :- 0.3294\n",
      "Epoch 79 , loss :- 0.3322\n",
      "Epoch 80 , loss :- 0.3324\n",
      "Epoch 81 , loss :- 0.3090\n",
      "Epoch 82 , loss :- 0.3000\n",
      "Epoch 83 , loss :- 0.2992\n",
      "Epoch 84 , loss :- 0.2950\n",
      "Epoch 85 , loss :- 0.3020\n",
      "Epoch 86 , loss :- 0.2965\n",
      "Epoch 87 , loss :- 0.2754\n",
      "Epoch 88 , loss :- 0.2889\n",
      "Epoch 89 , loss :- 0.2753\n",
      "Epoch 90 , loss :- 0.2717\n",
      "Epoch 91 , loss :- 0.2611\n",
      "Epoch 92 , loss :- 0.2504\n",
      "Epoch 93 , loss :- 0.2456\n",
      "Epoch 94 , loss :- 0.2473\n",
      "Epoch 95 , loss :- 0.2352\n",
      "Epoch 96 , loss :- 0.2326\n",
      "Epoch 97 , loss :- 0.2196\n",
      "Epoch 98 , loss :- 0.2115\n",
      "Epoch 99 , loss :- 0.2208\n",
      "Epoch 100 , loss :- 0.2186\n",
      "Fold 2 , Validation Loss: 0.7539\n",
      "Fold 3/5\n",
      "Epoch 1 , loss :- 1.0081\n",
      "Epoch 2 , loss :- 1.0053\n",
      "Epoch 3 , loss :- 0.9989\n",
      "Epoch 4 , loss :- 0.9818\n",
      "Epoch 5 , loss :- 0.9639\n",
      "Epoch 6 , loss :- 0.9444\n",
      "Epoch 7 , loss :- 0.9239\n",
      "Epoch 8 , loss :- 0.9020\n",
      "Epoch 9 , loss :- 0.8738\n",
      "Epoch 10 , loss :- 0.8466\n",
      "Epoch 11 , loss :- 0.8182\n",
      "Epoch 12 , loss :- 0.7875\n",
      "Epoch 13 , loss :- 0.7593\n",
      "Epoch 14 , loss :- 0.7321\n",
      "Epoch 15 , loss :- 0.7087\n",
      "Epoch 16 , loss :- 0.6909\n",
      "Epoch 17 , loss :- 0.6743\n",
      "Epoch 18 , loss :- 0.6462\n",
      "Epoch 19 , loss :- 0.6380\n",
      "Epoch 20 , loss :- 0.6238\n",
      "Epoch 21 , loss :- 0.6221\n",
      "Epoch 22 , loss :- 0.6374\n",
      "Epoch 23 , loss :- 0.6293\n",
      "Epoch 24 , loss :- 0.6287\n",
      "Epoch 25 , loss :- 0.6377\n",
      "Epoch 26 , loss :- 0.6415\n",
      "Epoch 27 , loss :- 0.6563\n",
      "Epoch 28 , loss :- 0.6365\n",
      "Epoch 29 , loss :- 0.6894\n",
      "Epoch 30 , loss :- 0.6929\n",
      "Epoch 31 , loss :- 0.7105\n",
      "Epoch 32 , loss :- 0.7307\n",
      "Epoch 33 , loss :- 0.6786\n",
      "Epoch 34 , loss :- 0.6860\n",
      "Epoch 35 , loss :- 0.6709\n",
      "Epoch 36 , loss :- 0.6605\n",
      "Epoch 37 , loss :- 0.6890\n",
      "Epoch 38 , loss :- 0.6280\n",
      "Epoch 39 , loss :- 0.6670\n",
      "Epoch 40 , loss :- 0.6113\n",
      "Epoch 41 , loss :- 0.6046\n",
      "Epoch 42 , loss :- 0.5920\n",
      "Epoch 43 , loss :- 0.5733\n",
      "Epoch 44 , loss :- 0.5797\n",
      "Epoch 45 , loss :- 0.5842\n",
      "Epoch 46 , loss :- 0.5862\n",
      "Epoch 47 , loss :- 0.5278\n",
      "Epoch 48 , loss :- 0.5257\n",
      "Epoch 49 , loss :- 0.4994\n",
      "Epoch 50 , loss :- 0.5127\n",
      "Epoch 51 , loss :- 0.5038\n",
      "Epoch 52 , loss :- 0.4780\n",
      "Epoch 53 , loss :- 0.4635\n",
      "Epoch 54 , loss :- 0.4671\n",
      "Epoch 55 , loss :- 0.4592\n",
      "Epoch 56 , loss :- 0.4557\n",
      "Epoch 57 , loss :- 0.4487\n",
      "Epoch 58 , loss :- 0.4293\n",
      "Epoch 59 , loss :- 0.4202\n",
      "Epoch 60 , loss :- 0.4066\n",
      "Epoch 61 , loss :- 0.4148\n",
      "Epoch 62 , loss :- 0.3957\n",
      "Epoch 63 , loss :- 0.3955\n",
      "Epoch 64 , loss :- 0.4065\n",
      "Epoch 65 , loss :- 0.4031\n",
      "Epoch 66 , loss :- 0.3769\n",
      "Epoch 67 , loss :- 0.3766\n",
      "Epoch 68 , loss :- 0.3553\n",
      "Epoch 69 , loss :- 0.3656\n",
      "Epoch 70 , loss :- 0.3929\n",
      "Epoch 71 , loss :- 0.3669\n",
      "Epoch 72 , loss :- 0.3388\n",
      "Epoch 73 , loss :- 0.3299\n",
      "Epoch 74 , loss :- 0.3383\n",
      "Epoch 75 , loss :- 0.3298\n",
      "Epoch 76 , loss :- 0.3392\n",
      "Epoch 77 , loss :- 0.3489\n",
      "Epoch 78 , loss :- 0.3261\n",
      "Epoch 79 , loss :- 0.3288\n",
      "Epoch 80 , loss :- 0.3236\n",
      "Epoch 81 , loss :- 0.3461\n",
      "Epoch 82 , loss :- 0.3179\n",
      "Epoch 83 , loss :- 0.3444\n",
      "Epoch 84 , loss :- 0.3115\n",
      "Epoch 85 , loss :- 0.3043\n",
      "Epoch 86 , loss :- 0.3086\n",
      "Epoch 87 , loss :- 0.2922\n",
      "Epoch 88 , loss :- 0.2921\n",
      "Epoch 89 , loss :- 0.2842\n",
      "Epoch 90 , loss :- 0.2851\n",
      "Epoch 91 , loss :- 0.2823\n",
      "Epoch 92 , loss :- 0.2698\n",
      "Epoch 93 , loss :- 0.2652\n",
      "Epoch 94 , loss :- 0.2666\n",
      "Epoch 95 , loss :- 0.2795\n",
      "Epoch 96 , loss :- 0.2921\n",
      "Epoch 97 , loss :- 0.3161\n",
      "Epoch 98 , loss :- 0.3299\n",
      "Epoch 99 , loss :- 0.3014\n",
      "Epoch 100 , loss :- 0.2476\n",
      "Fold 3 , Validation Loss: 0.6349\n",
      "Fold 4/5\n",
      "Epoch 1 , loss :- 1.0074\n",
      "Epoch 2 , loss :- 1.0054\n",
      "Epoch 3 , loss :- 1.0005\n",
      "Epoch 4 , loss :- 0.9878\n",
      "Epoch 5 , loss :- 0.9683\n",
      "Epoch 6 , loss :- 0.9430\n",
      "Epoch 7 , loss :- 0.9182\n",
      "Epoch 8 , loss :- 0.8889\n",
      "Epoch 9 , loss :- 0.8544\n",
      "Epoch 10 , loss :- 0.8155\n",
      "Epoch 11 , loss :- 0.7825\n",
      "Epoch 12 , loss :- 0.7492\n",
      "Epoch 13 , loss :- 0.7147\n",
      "Epoch 14 , loss :- 0.6998\n",
      "Epoch 15 , loss :- 0.6883\n",
      "Epoch 16 , loss :- 0.6770\n",
      "Epoch 17 , loss :- 0.6563\n",
      "Epoch 18 , loss :- 0.6326\n",
      "Epoch 19 , loss :- 0.6298\n",
      "Epoch 20 , loss :- 0.6140\n",
      "Epoch 21 , loss :- 0.6095\n",
      "Epoch 22 , loss :- 0.6013\n",
      "Epoch 23 , loss :- 0.6011\n",
      "Epoch 24 , loss :- 0.6218\n",
      "Epoch 25 , loss :- 0.5991\n",
      "Epoch 26 , loss :- 0.5873\n",
      "Epoch 27 , loss :- 0.5698\n",
      "Epoch 28 , loss :- 0.5604\n",
      "Epoch 29 , loss :- 0.5365\n",
      "Epoch 30 , loss :- 0.5623\n",
      "Epoch 31 , loss :- 0.5403\n",
      "Epoch 32 , loss :- 0.5597\n",
      "Epoch 33 , loss :- 0.5875\n",
      "Epoch 34 , loss :- 0.5697\n",
      "Epoch 35 , loss :- 0.6036\n",
      "Epoch 36 , loss :- 0.5592\n",
      "Epoch 37 , loss :- 0.5789\n",
      "Epoch 38 , loss :- 0.5700\n",
      "Epoch 39 , loss :- 0.5533\n",
      "Epoch 40 , loss :- 0.5363\n",
      "Epoch 41 , loss :- 0.5232\n",
      "Epoch 42 , loss :- 0.5345\n",
      "Epoch 43 , loss :- 0.5413\n",
      "Epoch 44 , loss :- 0.5475\n",
      "Epoch 45 , loss :- 0.5300\n",
      "Epoch 46 , loss :- 0.5458\n",
      "Epoch 47 , loss :- 0.5228\n",
      "Epoch 48 , loss :- 0.4993\n",
      "Epoch 49 , loss :- 0.5091\n",
      "Epoch 50 , loss :- 0.5180\n",
      "Epoch 51 , loss :- 0.5237\n",
      "Epoch 52 , loss :- 0.5434\n",
      "Epoch 53 , loss :- 0.5239\n",
      "Epoch 54 , loss :- 0.5166\n",
      "Epoch 55 , loss :- 0.4553\n",
      "Epoch 56 , loss :- 0.4547\n",
      "Epoch 57 , loss :- 0.4305\n",
      "Epoch 58 , loss :- 0.4399\n",
      "Epoch 59 , loss :- 0.4331\n",
      "Epoch 60 , loss :- 0.4237\n",
      "Epoch 61 , loss :- 0.4244\n",
      "Epoch 62 , loss :- 0.4175\n",
      "Epoch 63 , loss :- 0.3956\n",
      "Epoch 64 , loss :- 0.3827\n",
      "Epoch 65 , loss :- 0.3899\n",
      "Epoch 66 , loss :- 0.3661\n",
      "Epoch 67 , loss :- 0.3663\n",
      "Epoch 68 , loss :- 0.3556\n",
      "Epoch 69 , loss :- 0.3336\n",
      "Epoch 70 , loss :- 0.3241\n",
      "Epoch 71 , loss :- 0.3469\n",
      "Epoch 72 , loss :- 0.3172\n",
      "Epoch 73 , loss :- 0.3261\n",
      "Epoch 74 , loss :- 0.3175\n",
      "Epoch 75 , loss :- 0.3367\n",
      "Epoch 76 , loss :- 0.3059\n",
      "Epoch 77 , loss :- 0.3075\n",
      "Epoch 78 , loss :- 0.2906\n",
      "Epoch 79 , loss :- 0.2858\n",
      "Epoch 80 , loss :- 0.2902\n",
      "Epoch 81 , loss :- 0.2636\n",
      "Epoch 82 , loss :- 0.2605\n",
      "Epoch 83 , loss :- 0.2465\n",
      "Epoch 84 , loss :- 0.2555\n",
      "Epoch 85 , loss :- 0.2705\n",
      "Epoch 86 , loss :- 0.2726\n",
      "Epoch 87 , loss :- 0.2522\n",
      "Epoch 88 , loss :- 0.2502\n",
      "Epoch 89 , loss :- 0.2281\n",
      "Epoch 90 , loss :- 0.2196\n",
      "Epoch 91 , loss :- 0.2234\n",
      "Epoch 92 , loss :- 0.2158\n",
      "Epoch 93 , loss :- 0.2158\n",
      "Epoch 94 , loss :- 0.2066\n",
      "Epoch 95 , loss :- 0.2156\n",
      "Epoch 96 , loss :- 0.2166\n",
      "Epoch 97 , loss :- 0.2241\n",
      "Epoch 98 , loss :- 0.2067\n",
      "Epoch 99 , loss :- 0.1960\n",
      "Epoch 100 , loss :- 0.1839\n",
      "Fold 4 , Validation Loss: 0.6725\n",
      "Fold 5/5\n",
      "Epoch 1 , loss :- 1.0083\n",
      "Epoch 2 , loss :- 1.0058\n",
      "Epoch 3 , loss :- 0.9944\n",
      "Epoch 4 , loss :- 0.9729\n",
      "Epoch 5 , loss :- 0.9558\n",
      "Epoch 6 , loss :- 0.9368\n",
      "Epoch 7 , loss :- 0.9155\n",
      "Epoch 8 , loss :- 0.8923\n",
      "Epoch 9 , loss :- 0.8662\n",
      "Epoch 10 , loss :- 0.8385\n",
      "Epoch 11 , loss :- 0.8181\n",
      "Epoch 12 , loss :- 0.7922\n",
      "Epoch 13 , loss :- 0.7717\n",
      "Epoch 14 , loss :- 0.7530\n",
      "Epoch 15 , loss :- 0.7340\n",
      "Epoch 16 , loss :- 0.7119\n",
      "Epoch 17 , loss :- 0.7069\n",
      "Epoch 18 , loss :- 0.6896\n",
      "Epoch 19 , loss :- 0.6718\n",
      "Epoch 20 , loss :- 0.6681\n",
      "Epoch 21 , loss :- 0.6666\n",
      "Epoch 22 , loss :- 0.6359\n",
      "Epoch 23 , loss :- 0.6358\n",
      "Epoch 24 , loss :- 0.6321\n",
      "Epoch 25 , loss :- 0.6200\n",
      "Epoch 26 , loss :- 0.6118\n",
      "Epoch 27 , loss :- 0.6234\n",
      "Epoch 28 , loss :- 0.6085\n",
      "Epoch 29 , loss :- 0.6261\n",
      "Epoch 30 , loss :- 0.6316\n",
      "Epoch 31 , loss :- 0.6403\n",
      "Epoch 32 , loss :- 0.6442\n",
      "Epoch 33 , loss :- 0.6361\n",
      "Epoch 34 , loss :- 0.6509\n",
      "Epoch 35 , loss :- 0.6072\n",
      "Epoch 36 , loss :- 0.5685\n",
      "Epoch 37 , loss :- 0.5532\n",
      "Epoch 38 , loss :- 0.5729\n",
      "Epoch 39 , loss :- 0.5806\n",
      "Epoch 40 , loss :- 0.5608\n",
      "Epoch 41 , loss :- 0.5450\n",
      "Epoch 42 , loss :- 0.5519\n",
      "Epoch 43 , loss :- 0.5347\n",
      "Epoch 44 , loss :- 0.5469\n",
      "Epoch 45 , loss :- 0.5650\n",
      "Epoch 46 , loss :- 0.5832\n",
      "Epoch 47 , loss :- 0.5405\n",
      "Epoch 48 , loss :- 0.5479\n",
      "Epoch 49 , loss :- 0.5121\n",
      "Epoch 50 , loss :- 0.5928\n",
      "Epoch 51 , loss :- 0.5455\n",
      "Epoch 52 , loss :- 0.5331\n",
      "Epoch 53 , loss :- 0.5363\n",
      "Epoch 54 , loss :- 0.5165\n",
      "Epoch 55 , loss :- 0.4956\n",
      "Epoch 56 , loss :- 0.4796\n",
      "Epoch 57 , loss :- 0.4554\n",
      "Epoch 58 , loss :- 0.4789\n",
      "Epoch 59 , loss :- 0.4841\n",
      "Epoch 60 , loss :- 0.5153\n",
      "Epoch 61 , loss :- 0.5210\n",
      "Epoch 62 , loss :- 0.5394\n",
      "Epoch 63 , loss :- 0.5348\n",
      "Epoch 64 , loss :- 0.5528\n",
      "Epoch 65 , loss :- 0.5067\n",
      "Epoch 66 , loss :- 0.5126\n",
      "Epoch 67 , loss :- 0.5192\n",
      "Epoch 68 , loss :- 0.4899\n",
      "Epoch 69 , loss :- 0.4567\n",
      "Epoch 70 , loss :- 0.4509\n",
      "Epoch 71 , loss :- 0.4470\n",
      "Epoch 72 , loss :- 0.4051\n",
      "Epoch 73 , loss :- 0.4026\n",
      "Epoch 74 , loss :- 0.3709\n",
      "Epoch 75 , loss :- 0.3904\n",
      "Epoch 76 , loss :- 0.3652\n",
      "Epoch 77 , loss :- 0.3549\n",
      "Epoch 78 , loss :- 0.3678\n",
      "Epoch 79 , loss :- 0.3545\n",
      "Epoch 80 , loss :- 0.3397\n",
      "Epoch 81 , loss :- 0.3341\n",
      "Epoch 82 , loss :- 0.3287\n",
      "Epoch 83 , loss :- 0.3187\n",
      "Epoch 84 , loss :- 0.3110\n",
      "Epoch 85 , loss :- 0.3084\n",
      "Epoch 86 , loss :- 0.3038\n",
      "Epoch 87 , loss :- 0.2961\n",
      "Epoch 88 , loss :- 0.2828\n",
      "Epoch 89 , loss :- 0.2856\n",
      "Epoch 90 , loss :- 0.2733\n",
      "Epoch 91 , loss :- 0.2848\n",
      "Epoch 92 , loss :- 0.2748\n",
      "Epoch 93 , loss :- 0.2627\n",
      "Epoch 94 , loss :- 0.2549\n",
      "Epoch 95 , loss :- 0.2469\n",
      "Epoch 96 , loss :- 0.2384\n",
      "Epoch 97 , loss :- 0.2421\n",
      "Epoch 98 , loss :- 0.2490\n",
      "Epoch 99 , loss :- 0.2493\n",
      "Epoch 100 , loss :- 0.2620\n",
      "Fold 5 , Validation Loss: 0.6826\n",
      "Average Validation Loss across 5 folds: 0.6831\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Assuming `training_data` is your dataset\n",
    "k_folds = 5  # Number of folds\n",
    "kf = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
    "\n",
    "# Store results\n",
    "fold_results = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(training)):\n",
    "    print(f'Fold {fold+1}/{k_folds}')\n",
    "    \n",
    "    # Create training and validation sets\n",
    "    train_data = [training[i] for i in train_idx]\n",
    "    val_data = [training[i] for i in val_idx]\n",
    "\n",
    "    # Initialize your model, loss, optimizer, and scheduler\n",
    "    faceRecognition = FaceRecognition().to(get_device())\n",
    "    criterian = nn.TripletMarginLoss(margin=1.0, reduction='mean').to(get_device())\n",
    "    optimizer = torch.optim.SGD(params=faceRecognition.parameters(), lr=0.001, momentum=0.7, weight_decay=1e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=5, factor=0.7, verbose=True)\n",
    "    \n",
    "    for epoch in range(EPOCHS):\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        # Training loop\n",
    "        faceRecognition.train()\n",
    "        for i in range(0, len(train_data), BATCH_SIZE):\n",
    "            data = train_data[i:i+BATCH_SIZE]\n",
    "            anchor_tensor, positive_tensor, negative_tensor = dataFormate(data)\n",
    "\n",
    "            # Move data tensors to the correct device\n",
    "            anchor_tensor = anchor_tensor.to(get_device())\n",
    "            positive_tensor = positive_tensor.to(get_device())\n",
    "            negative_tensor = negative_tensor.to(get_device())\n",
    "\n",
    "            # Forward pass\n",
    "            anchor_embedding = faceRecognition(anchor_tensor)\n",
    "            positive_embedding = faceRecognition(positive_tensor)\n",
    "            negative_embedding = faceRecognition(negative_tensor)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = criterian(anchor_embedding, positive_embedding, negative_embedding)\n",
    "            \n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        avg_loss = running_loss / (len(train_data) // BATCH_SIZE)\n",
    "        scheduler.step(avg_loss)\n",
    "        print(f\"Epoch {epoch+1} , loss :- {avg_loss:.4f}\")\n",
    "\n",
    "    # Validation loop\n",
    "    faceRecognition.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(val_data), BATCH_SIZE):\n",
    "            data = val_data[i:i+BATCH_SIZE]\n",
    "            anchor_tensor, positive_tensor, negative_tensor = dataFormate(data)\n",
    "\n",
    "            # Move data tensors to the correct device\n",
    "            anchor_tensor = anchor_tensor.to(get_device())\n",
    "            positive_tensor = positive_tensor.to(get_device())\n",
    "            negative_tensor = negative_tensor.to(get_device())\n",
    "\n",
    "            # Forward pass\n",
    "            anchor_embedding = faceRecognition(anchor_tensor)\n",
    "            positive_embedding = faceRecognition(positive_tensor)\n",
    "            negative_embedding = faceRecognition(negative_tensor)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = criterian(anchor_embedding, positive_embedding, negative_embedding)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    avg_val_loss = val_loss / (len(val_data) // BATCH_SIZE)\n",
    "    print(f\"Fold {fold+1} , Validation Loss: {avg_val_loss:.4f}\")\n",
    "    \n",
    "    fold_results.append(avg_val_loss)\n",
    "\n",
    "# Calculate average validation loss across all folds\n",
    "average_val_loss = sum(fold_results) / k_folds\n",
    "print(f'Average Validation Loss across {k_folds} folds: {average_val_loss:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def calculate_distance(embedding1, embedding2):\n",
    "    return F.pairwise_distance(embedding1, embedding2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save only the model's state dictionary\n",
    "# torch.save(faceRecognition.state_dict(), 'face_recognition_triplet.pth')\n",
    "torch.save(faceRecognition.state_dict(), 'face_recognition_triplet2.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5, 1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0, 4.5, 5.0, 5.5, 6.0, 6.5, 7.0, 7.5, 8.0, 8.5, 9.0, 9.5, 10.0, 10.5, 11.0, 11.5, 12.0, 12.5, 13.0, 13.5, 14.0, 14.5, 15.0, 15.5, 16.0, 16.5, 17.0, 17.5, 18.0, 18.5, 19.0, 19.5, 20.0, 20.5, 21.0, 21.5, 22.0, 22.5, 23.0, 23.5, 24.0, 24.5, 25.0, 25.5, 26.0, 26.5, 27.0, 27.5, 28.0, 28.5, 29.0, 29.5, 30.0]\n",
      "Best Threshold: 0.5 with accuracy 0.5321620011911852\n"
     ]
    }
   ],
   "source": [
    "# Example: Evaluate on a validation dataset\n",
    "validation_pairs = [(anchor_tensor, positive_tensor), (anchor_tensor, negative_tensor)]  # Example pairs\n",
    "labels = [1, 0]  # 1 for same person, 0 for different persons\n",
    "validation_pairs, labels = [],[]\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in training:\n",
    "        anchor,positive,negative = dataFormate(data)\n",
    "        anchor_tensor = faceRecognition(anchor.unsqueeze(0))\n",
    "        positive_tensor = faceRecognition(positive.unsqueeze(0))\n",
    "        negative_tensor = faceRecognition(negative.unsqueeze(0))\n",
    "\n",
    "        validation_pairs.append((anchor_tensor,positive_tensor))\n",
    "        validation_pairs.append((anchor_tensor, negative_tensor))\n",
    "        labels.append(1)\n",
    "        labels.append(0)\n",
    "distances = []\n",
    "for (embedding1, embedding2) in validation_pairs:\n",
    "    distances.append(calculate_distance(embedding1, embedding2).item())\n",
    "\n",
    "# Try different thresholds and pick the best\n",
    "thresholds = []  # Example thresholds\n",
    "thr = 0.5\n",
    "while thr <= 30:\n",
    "    thresholds.append(thr)\n",
    "    thr += 0.5\n",
    "print(thresholds)\n",
    "best_threshold = 0\n",
    "best_accuracy = 0\n",
    "\n",
    "for threshold in thresholds:\n",
    "    predictions = [(d < threshold) for d in distances]\n",
    "    accuracy = sum([p == l for p, l in zip(predictions, labels)]) / len(labels)\n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        best_threshold = threshold\n",
    "\n",
    "print(f\"Best Threshold: {best_threshold} with accuracy {best_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
